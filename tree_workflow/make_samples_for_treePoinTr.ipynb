{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb18ba9",
   "metadata": {},
   "source": [
    "make point cloud samples for training treePointr models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c496e5a",
   "metadata": {},
   "source": [
    "### 1 make complete samples only for viewpoint method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92507368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import make_samples_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd84e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to folder with single tree point cloud files\n",
    "inpath = \"path/to/singletrees/\"\n",
    "# output path for samples\n",
    "outpath = \"path/for/output/\"\n",
    "\n",
    "# this function loops through all files (.npy, .xyz, .txt, .ply) in inpath and\n",
    "# makes the specified number of samples from each file:\n",
    "# make samples, specify start_count, stop_count, boxsize, and name of data source\n",
    "# creates samples while in range(stop_count)\n",
    "make_samples_complete.mksamples(inpath, outpath, stop_count=100, method='grove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_samples = \"path/to/samples/\" # outpath from before\n",
    "path = \"path/to/parentdirectory\"\n",
    "\n",
    "# divide into train and test sets by writing a txt file\n",
    "make_samples_complete.train_test_txt(path_samples, outpath=path, test_trees = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187b2da",
   "metadata": {},
   "source": [
    "### 2 make complete and partial samples from simulated scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fbaee1",
   "metadata": {},
   "source": [
    "Partial samples are based on the simulated scan positions (legs)\n",
    "\n",
    "This version is an update based on treePoinTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import make_samples_from_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "root = \"/home/katnips/treePoinTr/\"\n",
    "\n",
    "# path to folder with single tree complete point cloud files\n",
    "fulltree_path = root + \"data_brut/data_treepointr/TheGrove_pointclouds_npy/\" #TheGrove_pointclouds_npy/\n",
    "# path to folder with with individual legs of Helios++ simulations in .xyz format\n",
    "sim_path = root + \"data_brut/data_treepointr/HeliosSim/\"\n",
    "\n",
    "# output path for samples\n",
    "#outpath = root + \"data_training/tree_ada_v2_synthetic_8192_1/\"\n",
    "#outpath = root + \"data_training/tree_ada_v2_synthetic_2048_1/\"\n",
    "#outpath = root + \"data_training/tree_ada_v2_synthetic_2048_05/\"\n",
    "outpath = root + \"data_training/tree_ada_v2_synthetic_8192_05/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ba630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or generate centers for reproducibility\n",
    "# With a min_volume (0.5 m3) and num_samples (e.g., 200)\n",
    "\n",
    "num_samples = 200\n",
    "fulltree_path_npy = root + \"data_brut/data_treepointr/TheGrove_pointclouds_npy\" #TheGrove_pointclouds_npy/\n",
    "first_outpath = root + \"data_training/tree_ada_v2_synthetic_8192_1/\"\n",
    "\n",
    "if not os.path.exists(first_outpath):\n",
    "    os.makedirs(first_outpath)\n",
    "\n",
    "for file in os.listdir(fulltree_path_npy):\n",
    "    if file.endswith(\".npy\"):\n",
    "        treename = os.path.splitext(file)[0]\n",
    "        pc_path = os.path.join(fulltree_path_npy, file)\n",
    "        centers_path = os.path.join(first_outpath, f\"{treename}_centers.npy\")\n",
    "\n",
    "        if os.path.exists(centers_path):\n",
    "            print(f\"Loading existing centers for {treename}\")\n",
    "            centers = np.load(centers_path)\n",
    "        else:\n",
    "            print(f\"Generating centers for {treename}\")\n",
    "            pc = np.load(pc_path)\n",
    "            centers = make_samples_from_sim.generate_sample_centers(pc, num_samples=num_samples, cube_size=1, min_volume=0.5, save_path=centers_path)\n",
    "            print(f\"Generated {len(centers)} valid centers for {treename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cad34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samples from the generated centers \n",
    "\n",
    "num_samples = 200 # number of samples per tree\n",
    "nb_points = 8192 # 8192 or 2048 points for the downsampling \n",
    "boxsize = 0.3968  # 0.5 for 1m3 (cube size), 0.3968 for 0.5m3  \n",
    "\n",
    "for file in os.listdir(fulltree_path):\n",
    "    if file.endswith(\".npy\"):\n",
    "        treename = os.path.splitext(file)[0]\n",
    "        pc_path = os.path.join(fulltree_path, file)\n",
    "        centers_path = os.path.join(first_outpath, f\"{treename}_centers.npy\")\n",
    "\n",
    "        if os.path.exists(centers_path):\n",
    "            centers = np.load(centers_path)\n",
    "            print(f\"Loaded centers for {treename}\")\n",
    "        else:\n",
    "            pc = np.load(pc_path)\n",
    "            centers = make_samples_from_sim.generate_sample_centers(\n",
    "                pc, \n",
    "                num_samples=num_samples, \n",
    "                cube_size=boxsize,          \n",
    "                min_volume=0.5,             \n",
    "                save_path=centers_path\n",
    "            )\n",
    "            print(f\"Generated centers for {treename}\")\n",
    "\n",
    "        pc = np.load(pc_path)\n",
    "\n",
    "        print(f\"Start processing tree: {treename} with {len(centers)} centers\")\n",
    "\n",
    "        make_samples_from_sim.mksamples_sim_for_one_tree(\n",
    "            pc=pc,\n",
    "            sim_path=sim_path,\n",
    "            outpath=outpath,\n",
    "            centers=centers,\n",
    "            treename=treename,\n",
    "            stop_count=num_samples,\n",
    "            nb_points=nb_points,\n",
    "            boxsize=boxsize\n",
    "        )\n",
    "\n",
    "        print(f\"Finished processing tree: {treename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of partial samples  -> 15 max\n",
    "\n",
    "def keep_balanced_random_partial_variants(root_dir, max_total=15):\n",
    "    \"\"\"\n",
    "    For each subfolder in root_dir, keep up to max_total .npy files:\n",
    "    - Half with '_noise_downsample' in name\n",
    "    - Half without '_noise_downsample'\n",
    "    If not enough files exist, keep what’s available but preserve balance.\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        npy_files = [f for f in filenames if f.endswith('.npy')]\n",
    "        if len(npy_files) <= max_total:\n",
    "            continue  # Skip if already within the limit\n",
    "\n",
    "        # Split into noise and clean groups\n",
    "        noise_files = [f for f in npy_files if '_noise_downsample' in f]\n",
    "        clean_files = [f for f in npy_files if '_noise_downsample' not in f]\n",
    "\n",
    "        # Compute how many we can take from each group\n",
    "        half = max_total // 2\n",
    "        max_noise = min(half, len(noise_files))\n",
    "        max_clean = min(max_total - max_noise, len(clean_files))\n",
    "\n",
    "        # Adjust again if not enough in either group\n",
    "        if max_noise + max_clean < max_total:\n",
    "            # Try to top up from the other group if possible\n",
    "            remaining = max_total - (max_noise + max_clean)\n",
    "            if len(noise_files) > max_noise:\n",
    "                max_noise += min(remaining, len(noise_files) - max_noise)\n",
    "            elif len(clean_files) > max_clean:\n",
    "                max_clean += min(remaining, len(clean_files) - max_clean)\n",
    "\n",
    "        keep_noise = random.sample(noise_files, max_noise)\n",
    "        keep_clean = random.sample(clean_files, max_clean)\n",
    "        keep_set = set(keep_noise + keep_clean)\n",
    "\n",
    "        for f in npy_files:\n",
    "            if f not in keep_set:\n",
    "                os.remove(os.path.join(dirpath, f))\n",
    "                #print(f\"Removed: {os.path.join(dirpath, f)}\")\n",
    "\n",
    "# Usage\n",
    "partial_outpath = outpath + \"train/partial/\"\n",
    "keep_balanced_random_partial_variants(partial_outpath, max_total=15) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce the number of cubes for each dataset (5 000 cubes per dataset, with at least 1 partial sample per cube)\n",
    "\n",
    "def reduce_partial_samples(complete_dir, partial_dirs, max_partial_samples=5000, min_per_cube=1, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Step 1: Get all complete cubes\n",
    "    cubes = []\n",
    "    for root, _, files in os.walk(complete_dir):\n",
    "        for f in files:\n",
    "            if f.endswith(\".npy\"):\n",
    "                rel_path = os.path.relpath(os.path.join(root, f), complete_dir)\n",
    "                cube_id = os.path.splitext(rel_path)[0]\n",
    "                cubes.append(cube_id)\n",
    "\n",
    "    print(f\"Total complete cubes: {len(cubes)}\")\n",
    "\n",
    "    # Step 2: Collect all partial samples for each cube\n",
    "    cube_to_partials = defaultdict(list)\n",
    "    for pdir in partial_dirs:\n",
    "        for root, _, files in os.walk(pdir):\n",
    "            for f in files:\n",
    "                if f.endswith(\".npy\"):\n",
    "                    rel_dir = os.path.relpath(root, pdir)\n",
    "                    cube_id = rel_dir.replace(\"\\\\\", \"/\")  # normalize windows slashes\n",
    "                    cube_to_partials[cube_id].append(os.path.join(root, f))\n",
    "\n",
    "    total_partials_before = sum(len(v) for v in cube_to_partials.values())\n",
    "    print(f\"Total partial samples before reduction: {total_partials_before}\")\n",
    "\n",
    "    # Step 3: Keep at least `min_per_cube` partial samples per cube (if available)\n",
    "    guaranteed_partials = []\n",
    "    leftovers = []\n",
    "\n",
    "    for cube in cubes:\n",
    "        partials = cube_to_partials.get(cube, [])\n",
    "        if len(partials) <= min_per_cube:\n",
    "            guaranteed_partials.extend(partials)  # keep all if less than min\n",
    "        else:\n",
    "            # randomly select min_per_cube partials to keep\n",
    "            chosen = random.sample(partials, min_per_cube)\n",
    "            guaranteed_partials.extend(chosen)\n",
    "            # put the rest in leftovers\n",
    "            for p in partials:\n",
    "                if p not in chosen:\n",
    "                    leftovers.append(p)\n",
    "\n",
    "    print(f\"Guaranteed partial samples (min {min_per_cube} per cube): {len(guaranteed_partials)}\")\n",
    "\n",
    "    # Step 4: Randomly select additional partials from leftovers to fill up to max_partial_samples\n",
    "    remaining_quota = max_partial_samples - len(guaranteed_partials)\n",
    "    if remaining_quota > 0:\n",
    "        if len(leftovers) > remaining_quota:\n",
    "            extras = random.sample(leftovers, remaining_quota)\n",
    "        else:\n",
    "            extras = leftovers\n",
    "    else:\n",
    "        extras = []\n",
    "\n",
    "    # Step 5: Delete all partial files that are not in the keep list\n",
    "    to_keep = set(guaranteed_partials + extras)\n",
    "    removed_count = 0\n",
    "    kept_count = 0\n",
    "\n",
    "    for pdir in partial_dirs:\n",
    "        for root, _, files in os.walk(pdir):\n",
    "            for f in files:\n",
    "                if f.endswith(\".npy\"):\n",
    "                    full_path = os.path.join(root, f)\n",
    "                    if full_path not in to_keep:\n",
    "                        os.remove(full_path)\n",
    "                        removed_count += 1\n",
    "                    else:\n",
    "                        kept_count += 1\n",
    "\n",
    "    print(f\"Kept {kept_count} partial samples, removed {removed_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8bcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our function to reduce the number of partial samples\n",
    "\n",
    "points = str(8192)  # 8192 or 2048 points\n",
    "size = str(\"05\")    # 1 or 05 (m3)\n",
    "\n",
    "complete_dir = os.path.join(root, f\"data_training/tree_ada_v2_synthetic_{points}_{size}\", \"train\", \"complete\")\n",
    "partial_dirs = [os.path.join(root, f\"data_training/tree_ada_v2_synthetic_{points}_{size}\", \"train\", \"partial\")]\n",
    "\n",
    "reduce_partial_samples(\n",
    "    complete_dir=complete_dir,\n",
    "    partial_dirs=partial_dirs,\n",
    "    max_partial_samples=5000,\n",
    "    min_per_cube=1,\n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is ok with the partial and complete samples\n",
    "\n",
    "def check_partial_and_complete_info(complete_dir, partial_dirs):\n",
    "    print(f\"\\n--- Checking dataset in: {complete_dir} ---\")\n",
    "\n",
    "    # Count total partial npy files\n",
    "    total_partials = 0\n",
    "    for pdir in partial_dirs:\n",
    "        for root, _, files in os.walk(pdir):\n",
    "            total_partials += sum(f.endswith(\".npy\") for f in files)\n",
    "    print(f\"Total partial .npy files: {total_partials}\")\n",
    "\n",
    "    # Find one example complete .npy file\n",
    "    example_file = None\n",
    "    for root, _, files in os.walk(complete_dir):\n",
    "        for f in files:\n",
    "            if f.endswith(\".npy\"):\n",
    "                example_file = os.path.join(root, f)\n",
    "                break\n",
    "        if example_file:\n",
    "            break\n",
    "\n",
    "    if example_file:\n",
    "        pc = np.load(example_file)\n",
    "        n_points = pc.shape[0]\n",
    "        print(f\"Example complete file: {example_file}\")\n",
    "        print(f\"Number of points: {n_points}\")\n",
    "\n",
    "        # Compute bounding box size\n",
    "        mins = pc.min(axis=0)\n",
    "        maxs = pc.max(axis=0)\n",
    "        bbox_sizes = maxs - mins\n",
    "        volume = bbox_sizes[0] * bbox_sizes[1] * bbox_sizes[2]\n",
    "\n",
    "        print(f\"Bounding box sizes (X, Y, Z): {bbox_sizes}\")\n",
    "        print(f\"Calculated cube volume: {volume:.4f} m³\")\n",
    "    else:\n",
    "        print(\"No complete .npy files found\")\n",
    "\n",
    "# Root path\n",
    "root = \"/home/katnips/treePoinTr/\"\n",
    "\n",
    "# List of (complete_dir, partial_dirs) for each dataset\n",
    "datasets = [\n",
    "    (\"tree_ada_v2_synthetic_8192_1\",),\n",
    "    (\"tree_ada_v2_synthetic_2048_1\",),\n",
    "    (\"tree_ada_v2_synthetic_2048_05\",),\n",
    "    (\"tree_ada_v2_synthetic_8192_05\",)\n",
    "]\n",
    "\n",
    "for name in datasets:\n",
    "    complete_dir = os.path.join(root, \"data_training\", name[0], \"train\", \"complete\")\n",
    "    partial_dirs = [os.path.join(root, \"data_training\", name[0], \"train\", \"partial\")]\n",
    "    check_partial_and_complete_info(complete_dir, partial_dirs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ecc0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and visualize cube volumes from the complete samples\n",
    "\n",
    "# Function to compute cube volumes\n",
    "def collect_cube_volumes(dataset_name, complete_dir):\n",
    "    volumes = []\n",
    "    for root, _, files in os.walk(complete_dir):\n",
    "        for f in files:\n",
    "            if f.endswith(\".npy\"):\n",
    "                path = os.path.join(root, f)\n",
    "                try:\n",
    "                    pc = np.load(path)\n",
    "                    bbox = pc.max(axis=0) - pc.min(axis=0)\n",
    "                    volume = np.prod(bbox)\n",
    "                    volumes.append((dataset_name, volume))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {path}: {e}\")\n",
    "    return volumes\n",
    "\n",
    "# Dataset configuration\n",
    "root = \"/home/katnips/treePoinTr\"\n",
    "dataset_configs = [\n",
    "    (\"2048_1\", os.path.join(root, \"data_training/tree_ada_v2_synthetic_2048_1/train/complete\")),\n",
    "    (\"2048_05\", os.path.join(root, \"data_training/tree_ada_v2_synthetic_2048_05/train/complete\")),\n",
    "    (\"8192_1\", os.path.join(root, \"data_training/tree_ada_v2_synthetic_8192_1/train/complete\")),\n",
    "    (\"8192_05\", os.path.join(root, \"data_training/tree_ada_v2_synthetic_8192_05/train/complete\")),\n",
    "]\n",
    "\n",
    "# Collect volumes\n",
    "all_volumes = []\n",
    "for name, cdir in dataset_configs:\n",
    "    vols = collect_cube_volumes(name, cdir)\n",
    "    all_volumes.extend(vols)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_volumes, columns=[\"Dataset\", \"Volume\"])\n",
    "\n",
    "# Plot settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "ax = sns.histplot(\n",
    "    data=df,\n",
    "    x=\"Volume\",\n",
    "    hue=\"Dataset\",\n",
    "    element=\"step\",\n",
    "    stat=\"count\",\n",
    "    common_norm=False,\n",
    "    bins=50,\n",
    "    legend=True\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "#plt.title(\"Distribution of cube volumes per dataset\", fontsize=18)\n",
    "plt.xlabel(\"Cube volume (m³)\", fontsize=16)\n",
    "plt.ylabel(\"Number of cubes\", fontsize=16)\n",
    "ax.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "# Legend\n",
    "leg = ax.get_legend()\n",
    "if leg:\n",
    "    leg.set_title(\"Dataset\", prop={'size': 14})\n",
    "    for text in leg.get_texts():\n",
    "        text.set_fontsize(13)\n",
    "    leg.set_bbox_to_anchor((0.02, 0.98))\n",
    "    leg._loc = 2  # upper left\n",
    "\n",
    "# Layout and export\n",
    "plt.grid(True)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(\"/home/katnips/treePoinTr/figures/synthetic_dataset_cube_volumes_english.png\", dpi=600, bbox_inches='tight', pad_inches=0.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split : 80% train, 10% test, 10% val\n",
    "# and write a .json file listing all the train and test samples\n",
    "\n",
    "#outpath = root + \"/data_training/tree_ada_v2_synthetic_8192_1\"\n",
    "#outpath = root + \"/data_training/tree_ada_v2_synthetic_2048_1\"\n",
    "#outpath = root + \"/data_training/tree_ada_v2_synthetic_2048_05\"\n",
    "outpath = root + \"/data_training/tree_ada_v2_synthetic_8192_05\"\n",
    "\n",
    "# Directory containing the complete samples made before\n",
    "complete_dir_train = outpath+\"/train/complete/\" \n",
    "\n",
    "# Specify the name for the .json file \n",
    "dataset_name = \"tree_ada_v2_synthetic_8192_05\"\n",
    "\n",
    "# Use the function\n",
    "make_samples_from_sim.traintestval_json(complete_dir_train, outpath, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize (complete and partial)\n",
    "\n",
    "def plot_point_cloud(points, ax, title, color='b', s=0.4):\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=s, color=color, alpha=0.7)\n",
    "    ax.set_title(title, fontsize=8)\n",
    "    ax.axis('off')\n",
    "    ax.view_init(elev=30, azim=135)  # nouvelle vue 3D\n",
    "\n",
    "def extract_cube_prefix(name):\n",
    "    return \"_\".join(name.split(\"_\")[:2])\n",
    "\n",
    "dataset_info = {\n",
    "    \"tree_ada_v2_synthetic_2048_1/train\": \"2048 pts – 1m³\",\n",
    "    \"tree_ada_v2_synthetic_2048_05/train\": \"2048 pts – 0.5m³\",\n",
    "    \"tree_ada_v2_synthetic_8192_1/train\": \"8192 pts – 1m³\",\n",
    "    \"tree_ada_v2_synthetic_8192_05/train\": \"8192 pts – 0.5m³\",\n",
    "}\n",
    "\n",
    "root = \"/home/katnips/treePoinTr/data_training\"\n",
    "dataset_dirs = list(dataset_info.keys())\n",
    "\n",
    "# Step 1: Find common cube prefixes\n",
    "dataset_complete_cubes = []\n",
    "for dataset_path in dataset_dirs:\n",
    "    complete_dir = os.path.join(root, dataset_path, \"complete\")\n",
    "    valid_cubes = set()\n",
    "    for tree_name in os.listdir(complete_dir):\n",
    "        tree_dir = os.path.join(complete_dir, tree_name)\n",
    "        if not os.path.isdir(tree_dir):\n",
    "            continue\n",
    "        for file in os.listdir(tree_dir):\n",
    "            if file.endswith(\".npy\"):\n",
    "                cube_prefix = extract_cube_prefix(os.path.splitext(file)[0])\n",
    "                valid_cubes.add((tree_name, cube_prefix))\n",
    "    dataset_complete_cubes.append(valid_cubes)\n",
    "\n",
    "common_complete_cubes = set.intersection(*dataset_complete_cubes)\n",
    "if not common_complete_cubes:\n",
    "    print(\"❌ No common cube prefix found.\")\n",
    "    exit()\n",
    "\n",
    "selected_tree, selected_prefix = sorted(common_complete_cubes)[0]\n",
    "print(f\"✔ Selected cube: {selected_tree}/{selected_prefix}\")\n",
    "\n",
    "# Step 2: Load point clouds\n",
    "pointcloud_pairs = []\n",
    "for dataset_path in dataset_dirs:\n",
    "    label = dataset_info[dataset_path]\n",
    "    complete_dir = os.path.join(root, dataset_path, \"complete\", selected_tree)\n",
    "    partial_dir = os.path.join(root, dataset_path, \"partial\", selected_tree)\n",
    "\n",
    "    complete_file = None\n",
    "    for f in os.listdir(complete_dir):\n",
    "        if f.endswith(\".npy\") and extract_cube_prefix(f) == selected_prefix:\n",
    "            complete_file = os.path.join(complete_dir, f)\n",
    "            break\n",
    "\n",
    "    if not complete_file:\n",
    "        continue\n",
    "\n",
    "    matching_cube_folder = None\n",
    "    for folder in os.listdir(partial_dir):\n",
    "        if extract_cube_prefix(folder) == selected_prefix:\n",
    "            matching_cube_folder = folder\n",
    "            break\n",
    "\n",
    "    if not matching_cube_folder:\n",
    "        continue\n",
    "\n",
    "    partial_files = [f for f in os.listdir(os.path.join(partial_dir, matching_cube_folder))\n",
    "                     if f.endswith(\".npy\") and \"_noise_downsample\" not in f]\n",
    "\n",
    "    if not partial_files:\n",
    "        continue\n",
    "\n",
    "    complete_pc = np.load(complete_file)\n",
    "    partial_pc = np.load(os.path.join(partial_dir, matching_cube_folder, partial_files[0]))\n",
    "    pointcloud_pairs.append((label, complete_pc, partial_pc))\n",
    "\n",
    "# Step 3: Plot\n",
    "n = len(pointcloud_pairs)\n",
    "fig = plt.figure(figsize=(3.2 * n, 5), dpi=300)\n",
    "\n",
    "for idx, (label, complete_pc, partial_pc) in enumerate(pointcloud_pairs):\n",
    "    ax1 = fig.add_subplot(2, n, idx + 1, projection='3d')\n",
    "    plot_point_cloud(complete_pc, ax1, f\"{label}\\nComplete\", color='c')\n",
    "\n",
    "    ax2 = fig.add_subplot(2, n, n + idx + 1, projection='3d')\n",
    "    plot_point_cloud(partial_pc, ax2, \"Partial\", color='#a855f7')  # violet clair\n",
    "\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.subplots_adjust(top=0.98, wspace=0.05, hspace=0.05)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates variants of point cloud samples by adding noise and downsampling\n",
    "# Useful for data augmentation, as a second part of the training (idea of curriculum learning)\n",
    "# It is a parallel dataset to the original one, with the same structure\n",
    "\n",
    "def add_noise(points, noise_level=0.01):\n",
    "    noise = np.random.normal(scale=noise_level, size=points.shape)\n",
    "    return points + noise\n",
    "\n",
    "def downsample(points, ratio=0.5):\n",
    "    n_samples = int(points.shape[0] * ratio)\n",
    "    if n_samples == 0:\n",
    "        return points\n",
    "    indices = np.random.choice(points.shape[0], n_samples, replace=False)\n",
    "    return points[indices]\n",
    "\n",
    "def create_noisy_parallel_dataset(src_root, dst_root, noise_level=0.01, downsample_ratio=0.5):\n",
    "    splits = ['train', 'val', 'test']\n",
    "\n",
    "    for split in splits:\n",
    "        src_partial_dir = os.path.join(src_root, split, 'partial')\n",
    "        dst_partial_dir = os.path.join(dst_root, split, 'partial')\n",
    "\n",
    "        if not os.path.exists(src_partial_dir):\n",
    "            print(f\"[{split}] Source partial directory does not exist: {src_partial_dir}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[{split}] Processing partials from {src_partial_dir}...\")\n",
    "\n",
    "        # Walk one more level to reach actual .npy files\n",
    "        for tree_name in os.listdir(src_partial_dir):\n",
    "            tree_path = os.path.join(src_partial_dir, tree_name)\n",
    "            if not os.path.isdir(tree_path):\n",
    "                continue\n",
    "\n",
    "            # Now iterate over cube folders (like ash1_4_size1.0)\n",
    "            for cube_folder in os.listdir(tree_path):\n",
    "                cube_path = os.path.join(tree_path, cube_folder)\n",
    "                if not os.path.isdir(cube_path):\n",
    "                    continue\n",
    "\n",
    "                rel_path = os.path.relpath(cube_path, src_partial_dir)\n",
    "                dst_dir = os.path.join(dst_partial_dir, rel_path)\n",
    "                os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "                # Process all .npy files inside this cube folder\n",
    "                for filename in os.listdir(cube_path):\n",
    "                    if filename.endswith('.npy') and ('noise' not in filename) and ('downsample' not in filename):\n",
    "                        src_file = os.path.join(cube_path, filename)\n",
    "                        points = np.load(src_file)\n",
    "\n",
    "                        noisy_points = add_noise(points, noise_level)\n",
    "                        noisy_down_points = downsample(noisy_points, downsample_ratio)\n",
    "\n",
    "                        dst_file = os.path.join(dst_dir, filename)\n",
    "                        np.save(dst_file, noisy_down_points)\n",
    "\n",
    "        print(f\"[{split}] Done.\")\n",
    "\n",
    "    print(\"Parallel noisy dataset creation completed.\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "def copy_complete_and_json(src_root, dst_root):\n",
    "    # Copy complete folders recursively\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        src_complete = os.path.join(src_root, split, \"complete\")\n",
    "        dst_complete = os.path.join(dst_root, split, \"complete\")\n",
    "        if os.path.exists(src_complete):\n",
    "            shutil.copytree(src_complete, dst_complete, dirs_exist_ok=True)\n",
    "            print(f\"Copied complete data: {src_complete} -> {dst_complete}\")\n",
    "        else:\n",
    "            print(f\"Complete folder not found: {src_complete}\")\n",
    "\n",
    "    # Copy JSON file(s) if exist\n",
    "    for f in os.listdir(src_root):\n",
    "        if f.endswith(\".json\"):\n",
    "            src_json = os.path.join(src_root, f)\n",
    "            dst_json = os.path.join(dst_root, f)\n",
    "            shutil.copy2(src_json, dst_json)\n",
    "            print(f\"Copied JSON file: {src_json} -> {dst_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5871e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to create a parallel noisy dataset\n",
    "src_root = \"/home/katnips/treePoinTr/data_training/after_split_without_noise/tree_ada_v2_synthetic_8192_1\"\n",
    "dst_root = \"/home/katnips/treePoinTr/data_training/after_split_with_noise/tree_ada_v2_synthetic_8192_1_noisy\"\n",
    "\n",
    "create_noisy_parallel_dataset(\n",
    "    src_root=src_root,\n",
    "    dst_root=dst_root,\n",
    "    noise_level=0.007,       # noise std dev\n",
    "    downsample_ratio=0.7     # keep 70% points after downsampling\n",
    ")\n",
    "\n",
    "copy_complete_and_json(src_root, dst_root)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the point clouds, including the noisy ones\n",
    "\n",
    "def plot_point_cloud(points, ax, color='b', s=0.4):\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], s=s, color=color, alpha=0.7)\n",
    "    ax.axis('off')\n",
    "    ax.view_init(elev=30, azim=135)\n",
    "\n",
    "def extract_cube_prefix(name):\n",
    "    return \"_\".join(name.split(\"_\")[:2])\n",
    "\n",
    "# Datasets\n",
    "base_dir_clean = \"/home/katnips/treePoinTr/data_training/after_split_without_noise\"\n",
    "base_dir_noisy = \"/home/katnips/treePoinTr/data_training/after_split_with_noise\"\n",
    "\n",
    "dataset_info = {\n",
    "    \"tree_ada_v2_synthetic_2048_1\": \"2048 pts – 1m³\",\n",
    "    \"tree_ada_v2_synthetic_2048_05\": \"2048 pts – 0.5m³\",\n",
    "    \"tree_ada_v2_synthetic_8192_1\": \"8192 pts – 1m³\",\n",
    "    \"tree_ada_v2_synthetic_8192_05\": \"8192 pts – 0.5m³\",\n",
    "}\n",
    "\n",
    "dataset_dirs = list(dataset_info.keys())\n",
    "dataset_complete_cubes = []\n",
    "\n",
    "# Trouver un cube commun à tous les datasets\n",
    "for dataset_name in dataset_dirs:\n",
    "    complete_dir = os.path.join(base_dir_clean, dataset_name, \"train\", \"complete\")\n",
    "    valid_cubes = set()\n",
    "    for tree_name in os.listdir(complete_dir):\n",
    "        tree_path = os.path.join(complete_dir, tree_name)\n",
    "        if not os.path.isdir(tree_path):\n",
    "            continue\n",
    "        for fname in os.listdir(tree_path):\n",
    "            if fname.endswith(\".npy\"):\n",
    "                prefix = extract_cube_prefix(fname)\n",
    "                valid_cubes.add((tree_name, prefix))\n",
    "    dataset_complete_cubes.append(valid_cubes)\n",
    "\n",
    "common_complete_cubes = set.intersection(*dataset_complete_cubes)\n",
    "if not common_complete_cubes:\n",
    "    print(\"❌ No common cubes found.\")\n",
    "    exit()\n",
    "\n",
    "selected_tree, selected_prefix = sorted(common_complete_cubes)[0]\n",
    "print(f\"✔ Selected cube: {selected_tree}/{selected_prefix}\")\n",
    "\n",
    "# Charger les triplets de point clouds\n",
    "pointcloud_triplets = []\n",
    "\n",
    "for dataset_name in dataset_dirs:\n",
    "    label = dataset_info[dataset_name]\n",
    "\n",
    "    clean_complete_dir = os.path.join(base_dir_clean, dataset_name, \"train\", \"complete\", selected_tree)\n",
    "    clean_partial_dir = os.path.join(base_dir_clean, dataset_name, \"train\", \"partial\", selected_tree)\n",
    "    noisy_partial_dir = os.path.join(base_dir_noisy, dataset_name + \"_noisy\", \"train\", \"partial\", selected_tree)\n",
    "\n",
    "    complete_file = None\n",
    "    for f in os.listdir(clean_complete_dir):\n",
    "        if f.endswith(\".npy\") and extract_cube_prefix(f) == selected_prefix:\n",
    "            complete_file = os.path.join(clean_complete_dir, f)\n",
    "            break\n",
    "    if not complete_file:\n",
    "        continue\n",
    "\n",
    "    matching_folder = None\n",
    "    for folder in os.listdir(clean_partial_dir):\n",
    "        if extract_cube_prefix(folder) == selected_prefix:\n",
    "            matching_folder = folder\n",
    "            break\n",
    "    if not matching_folder:\n",
    "        continue\n",
    "\n",
    "    clean_partial_folder = os.path.join(clean_partial_dir, matching_folder)\n",
    "    noisy_partial_folder = os.path.join(noisy_partial_dir, matching_folder)\n",
    "\n",
    "    partial_files = [f for f in os.listdir(clean_partial_folder) if f.endswith(\".npy\")]\n",
    "    if not partial_files:\n",
    "        continue\n",
    "\n",
    "    partial_file = partial_files[0]\n",
    "    clean_partial_file = os.path.join(clean_partial_folder, partial_file)\n",
    "    noisy_partial_file = os.path.join(noisy_partial_folder, partial_file)\n",
    "    has_noisy = os.path.exists(noisy_partial_file)\n",
    "\n",
    "    complete_pc = np.load(complete_file)\n",
    "    partial_pc = np.load(clean_partial_file)\n",
    "    noisy_pc = np.load(noisy_partial_file) if has_noisy else None\n",
    "\n",
    "    pointcloud_triplets.append((label, complete_pc, partial_pc, noisy_pc))\n",
    "\n",
    "# Affichage\n",
    "n = len(pointcloud_triplets)\n",
    "fig, axes = plt.subplots(nrows=3, ncols=n, figsize=(3.2 * n, 7), subplot_kw={\"projection\": \"3d\"}, dpi=300)\n",
    "\n",
    "row_labels = [\n",
    "    \"Nuage de points complet\", \n",
    "    \"Partiel (sans bruit)\", \n",
    "    \"Partiel (avec bruit)\"\n",
    "]\n",
    "\n",
    "for col, (label, complete_pc, partial_pc, noisy_pc) in enumerate(pointcloud_triplets):\n",
    "    plot_point_cloud(complete_pc, axes[0, col], color='c')\n",
    "    plot_point_cloud(partial_pc, axes[1, col], color='#a855f7')\n",
    "    if noisy_pc is not None:\n",
    "        plot_point_cloud(noisy_pc, axes[2, col], color='#f472b6')\n",
    "    else:\n",
    "        axes[2, col].text(0, 0, 0, \"Missing\", ha='center')\n",
    "        axes[2, col].axis('off')\n",
    "\n",
    "    # Ajouter le titre du modèle en haut\n",
    "    axes[0, col].set_title(label, fontsize=9)\n",
    "\n",
    "# Ajouter les labels de ligne à gauche\n",
    "for row in range(3):\n",
    "    axes[row, 0].text2D(-0.15, 0.5, row_labels[row], transform=axes[row, 0].transAxes,\n",
    "                        fontsize=10, ha='right', va='center', rotation=90)\n",
    "\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.subplots_adjust(top=0.92, left=0.07, wspace=0.05, hspace=0.05)\n",
    "plt.savefig(\"/home/katnips/treePoinTr/figures/synthetic_dataset.png\", dpi=600, bbox_inches='tight', pad_inches=0.05)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154407b",
   "metadata": {},
   "source": [
    "### 3 make complete and partial samples from real scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679aeda",
   "metadata": {},
   "source": [
    "partial samples are based on individual scan positions (PointSourceId) or alternatively split by GpsTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import make_samples_from_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ebb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder containing single tree files .xyz format that include the columns 'PointSourceId' or 'GpsTime'\n",
    "fulltree_path = \"path/to/singletrees/\"\n",
    "# output path for samples\n",
    "outpath = \"path/for/output/\"\n",
    "\n",
    "# this function loops through all files (.xyz) in inpath and\n",
    "# makes the specified number of complete samples from each file including partial samples in a separate directory:\n",
    "# make samples, specify start_count, stop_count, boxsize,\n",
    "# creates samples while in range(stop_count)\n",
    "make_samples_from_real.mksamples_real(fulltree_path, outpath, start_count=0, stop_count=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd14444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing the complete samples made before\n",
    "complete_dir_train = outpath+\"/train/complete/\" \n",
    "# specify the name for the .json file (should be the dataset name)\n",
    "dataset_name = \"real\"\n",
    "\n",
    "\n",
    "# move a random 20% of the created data to a new directory called \"test\"\n",
    "# and write a .json file listing all the train and test samples\n",
    "make_samples_from_real.traintest_json(complete_dir_train, outpath, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_treepointr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
