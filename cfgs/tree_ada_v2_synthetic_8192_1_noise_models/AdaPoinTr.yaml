optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.0001, # Don t try 0.001, it's not working 
  weight_decay : 0.0005
}}

scheduler: {
  type: LambdaLR,
  kwargs: {
  decay_step: 21,
  lr_decay: 0.9,
  lowest_decay: 0.02  # min lr = lowest_decay * lr
}}

bnmscheduler: {
  type: Lambda,
  kwargs: {
  decay_step: 21,
  bn_decay: 0.5,
  bn_momentum: 0.9,
  lowest_decay: 0.01
}}

dataset : {
  train : { _base_: cfgs/dataset_configs/tree_ada_v2_synthetic_8192_1_noise.yaml, 
            others: {subset: 'train'}},
  val : { _base_: cfgs/dataset_configs/tree_ada_v2_synthetic_8192_1_noise.yaml, 
            others: {subset: 'test'}},
  test : { _base_: cfgs/dataset_configs/tree_ada_v2_synthetic_8192_1_noise.yaml, 
            others: {subset: 'test'}}}     
            
model : {
    NAME: AdaPoinTr, 
    num_query: 512, 
    num_points: 8192, # or 2048 for other datasets
    center_num: [512, 256],
    global_feature_dim: 1024, 
    encoder_type: graph,
    decoder_type: fc,
    encoder_config: {
      embed_dim: 384,
      depth: 6,
      num_heads: 6,
      k: 8,
      n_group: 2,
      mlp_ratio: 2.,
      block_style_list: ['attn-graph', 'attn', 'attn', 'attn', 'attn', 'attn'], 
      combine_style: 'concat',
    },
    decoder_config: {
      embed_dim: 384,
      depth: 8,
      num_heads: 6,
      k: 8,
      n_group: 2,
      mlp_ratio: 2.,
      self_attn_block_style_list: ['attn-graph', 'attn', 'attn', 'attn', 'attn', 'attn', 'attn', 'attn'], 
      self_attn_combine_style: 'concat',
      cross_attn_block_style_list: ['attn-graph', 'attn', 'attn', 'attn', 'attn', 'attn', 'attn', 'attn'], 
      cross_attn_combine_style: 'concat',
    }
}
  
total_bs : 48               # If the GPU has only 8GB GPU memory : small tests with bs 6 and step_per_update 3 / else : bs 48 and step_per_update 1
step_per_update : 1    
max_epoch : 600            

# Evaluation metric
consider_metric: CDL1      # Use Chamfer Distance L1 (for l2, you have to edit the model directly)
